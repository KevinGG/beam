{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Streaming Word Count\n",
    "\n",
    "This example demonstrates how to set up a streaming processing pipeline that reads from a\n",
    "[Google Pub/Sub](https://cloud.google.com/pubsub) topic. Each message in the Pub/Sub topic is a word from Shakespeare's work *King Lear*, \n",
    "The pipeline performs a frequency count on each of those words by window. \n",
    "\n",
    "You'll be able to use this notebook to explore the data in each PCollection.\n",
    "\n",
    "Before you start, please ensure the PubSub API is enabled [here](https://console.cloud.google.com/apis/library/pubsub.googleapis.com).\n",
    "\n",
    "We start with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive import interactive_runner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from datetime import timedelta\n",
    "import google.auth\n",
    "\n",
    "# The Google Cloud PubSub topic that we are reading from for this example.\n",
    "topic = \"projects/pubsub-public-data/topics/shakespeare-kinglear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are setting up the options to create the streaming pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(pipeline_options.StandardOptions).streaming = True\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "# The project will be used for creating a subscription to the Pub/Sub topic.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading from Google Cloud Pub/Sub, which is an unbounded source. By default, *Apache Beam Notebooks* will capture\n",
    "data from the unbounded sources for replayability. \n",
    "\n",
    "The following sets the data capture duration to 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.options.capture_duration = timedelta(seconds=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a pipeline with the *Interactive Runner* as the runner with the options we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(interactive_runner.InteractiveRunner(), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a `PTransform` that will create a subscription to the given Pub/Sub topic and reads from the subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are reading from an unbounded source, we need to create a windowing scheme so that we can\n",
    "count the words by window. The following creates fixed windowing with each window being 10 seconds in duration.\n",
    "For more information about windowing in Apache Beam, please visit the [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/#windowing-basics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_words = (words \n",
    "                  | \"window\" >> beam.WindowInto(beam.window.FixedWindows(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `PTransform` will count the words by window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_word_counts = (windowed_words\n",
    "                        | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ib.show()` method takes a `PCollection` as a parameter, runs the pipeline that contributes to it, and\n",
    "shows its content as data comes in. This method will return when all the data has been read.\n",
    "\n",
    "The optional parameter `include_window_info=True` will include the window information for each element in the output.\n",
    "You will see 3 additional columns: `event_time`, `windows`, and `pane_info`.\n",
    "`event_time` is the timestamp associated with the value.\n",
    "`windows` in this example tells you the start timestamp of the window and its duration.\n",
    "`pane_info` describes the [triggering](https://beam.apache.org/documentation/programming-guide/#triggers) information for the pane that contained the value.\n",
    "\n",
    "This example does not use custom triggering so by default there will be only one pane per window labeled `Pane 0`.\n",
    "\n",
    "Note that this also automatically captures a bounded segment of the unbounded source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_word_counts, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have captured a bounded segment of the unbounded source, the following will show the same data\n",
    "as the previous `ib.show()` call. This is to ensure replayability so that you can iteratively augment\n",
    "your pipeline and verify the output with the same input, which you will see in future cells in this notebook.\n",
    "Note the parameter `visualize_data=True`. This optional parameter gives you a visualization of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_word_counts, include_window_info=True, visualize_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, to ensure replayability for iterative prototyping of your pipeline,\n",
    "`ib.show()` calls will reuse the captured data by default. You can change this behavior and\n",
    "have it always fetch new data, by doing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this only if you would like to change the replay behavior:\n",
    "# ib.options.enable_capture_replay = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `PTransform` will count the words in lowercase by window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_lower_word_counts = (windowed_words\n",
    "                              | beam.Map(lambda word: word.lower())\n",
    "                              | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have not changed `ib.options.enable_capture_replay`, the following will return the count using the same words \n",
    "as before but with lowercase.\n",
    "Because all words are converted to lowercase before being counted, some words will have a higher count than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_lower_word_counts, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives you a [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that represents the `PCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.collect(windowed_lower_word_counts, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the first example, this example is designed to run easily on a single machine. If the input stream had a very high volume, you'd want to add an output sink to your PCollection result by doing something like:\n",
    "```\n",
    "windowed_lower_word_counts | beam.io.<some output transform>\n",
    "```\n",
    "and let [Google Cloud Dataflow](https://cloud.google.com/dataflow) run your pipeline.\n",
    "\n",
    "You can find the list of built-in input and output transforms [here](https://beam.apache.org/documentation/io/built-in/).\n",
    "\n",
    "Please refer to the user guide <TODO: URL> on how to run a Dataflow job using a pipeline assembled from your notebook. You can also refer to [this walkthrough](Dataflow_Word_Count.ipynb) which is based on the [first word count example notebook](01-Word_Count.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
