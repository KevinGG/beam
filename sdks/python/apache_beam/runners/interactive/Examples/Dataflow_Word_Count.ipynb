{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the examples on Google Cloud Dataflow\n",
    "\n",
    "There is a procedure on how to convert a pipeline assembled in the notebook environment to a pipeline suitable to be run in Dataflow in the guide <TODO: URL>. This notebook illustrates how the pipeline in the [First Word Count example](01-Word_Count.ipynb) can be run with the Dataflow Runner, instead of the Interactive Runner.\n",
    "\n",
    "Before you start, please ensure the Dataflow API is enabled [here](https://console.cloud.google.com/apis/library/dataflow.googleapis.com).\n",
    "\n",
    "### Assembling the pipeline for Dataflow Runner\n",
    "\n",
    "First, we remove all \"interactive\" imports and import the Dataflow runner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up pipeline options\n",
    "\n",
    "We now set up the pipeline options for running in Dataflow. For details on the pipeline options for Dataflow, please visit the [Cloud Dataflow documentation](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow will run.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-central1'\n",
    "\n",
    "# Tells Dataflow that we are running the job from a notebook environment.\n",
    "options.view_as(GoogleCloudOptions).labels = (\n",
    "    ['goog-dataflow-notebook=' + beam.version.__version__.replace('.', '_')])\n",
    "\n",
    "# Because this notebook comes with a locally built version of the Beam Python SDK, we will need to set\n",
    "# the sdk_location option for the Dataflow Runner. You will not need to do this if you are using an\n",
    "# officially released version of Apache Beam.\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' % \n",
    "    beam.version.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*IMPORTANT*: Please adjust the following code to choose a Google Cloud Storage (GCS) location for Dataflow files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Please adjust the following to choose a GCS location.\n",
    "dataflow_gcs_location = 'gs://<my_bucket>/dataflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The staging location is for storing files that will be copied to Dataflow workers, including code that will be executed by Dataflow workers. The temporary location is for storing temporary files generated by the Dataflow job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataflow Staging Location.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Dataflow Temp Location.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding PTransforms and PCollections to the pipeline\n",
    "\n",
    "The following class definition is the same as the one in the [original Word Count](01-Word_Count.ipynb) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordsFromText(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, file_pattern):\n",
    "        self._file_pattern = file_pattern\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll.pipeline\n",
    "                | beam.io.ReadFromText(self._file_pattern)\n",
    "                | beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates an Apache Beam pipeline with the *Dataflow Runner*, instead of the Interactive Runner in the original example, with the options we just set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(DataflowRunner(), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assembles the pipeline. It is the same as the original example, but with all the interactive calls removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | 'read' >> ReadWordsFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')\n",
    "counts = words | 'count' >> beam.combiners.Count.PerElement()\n",
    "lower_counts = (words\n",
    "                | \"lower\" >> beam.Map(lambda word: word.lower())\n",
    "                | \"lower_count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the results\n",
    "\n",
    "Now, we want to write the results, contained in the PCollections `counts` and `lower_counts`, to GCS files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to store the output files of the job.\n",
    "output_gcs_location = '%s/output' % dataflow_gcs_location\n",
    "\n",
    "# Specifying the GCS location to write `counts` to,\n",
    "# based on the `output_gcs_location` variable set earlier.\n",
    "(counts | 'Write counts to GCS' \n",
    " >> beam.io.WriteToText(output_gcs_location + '/wordcount-output.txt'))\n",
    "\n",
    "# Specifying the GCS location to write `lower_counts` to,\n",
    "# based on the `output_gcs_location` variable set earlier.\n",
    "(lower_counts | 'Write lower counts to GCS' \n",
    " >> beam.io.WriteToText(output_gcs_location + '/wordcount-lower-output.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline\n",
    "\n",
    "Now we are ready to run the pipeline on Dataflow. `p.run()` will run the pipeline and return a pipeline result object. You can ignore the warnings this gives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pipeline_result` handle, the following code builds a link to the Google Cloud Console web page that shows you details of the Dataflow job you just started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for the job to finish. The following call will block until the job is finished. It will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results\n",
    "\n",
    "Now that the job is finished, we can check the results in GCS using the [`gsutil`](https://cloud.google.com/storage/docs/gsutil) command-line tool. Note that `beam.io.WriteToText` writes the results in a sharded set of output files. For example, if the output is specified as `gs://my_bucket/output_directory/result.txt`, the results will be written in files with names like `gs://my_bucket/output_directory/result.txt-<shard>-of-<number-of-shards>`. Let's check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {output_gcs_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the content of the files by looking at the first 10 lines of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {output_gcs_location}/wordcount-output.txt* | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {output_gcs_location}/wordcount-lower-output.txt* | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Using this technique, you can also try launching Dataflow jobs for other examples listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
