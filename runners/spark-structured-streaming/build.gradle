/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * License); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import groovy.json.JsonOutput

apply plugin: org.apache.beam.gradle.BeamModulePlugin
applyJavaNature()

description = "Apache Beam :: Runners :: Spark-Structured-Streaming"

/*
 * We need to rely on manually specifying these evaluationDependsOn to ensure that
 * the following projects are evaluated before we evaluate this project. This is because
 * we are attempting to reference the "sourceSets.test.output" directly.
 */
evaluationDependsOn(":beam-sdks-java-core")

configurations {
  validatesRunner
}

configurations.all {
  resolutionStrategy {
    // Beam parent forces all deps to be the versions defined. In our case we have spark-core
    // defined to v2.3.2 for "old" spark runner. So spark-core transitive dep of spark-sql v2.4.0
    // gets forced to 2.3.2. Remove it from the forced deps and add v 2.4.0
//    forcedModules.removeElement("org.apache.spark:spark-core_2.11:2.3.2")
//    forcedModules.removeElement("org.apache.spark:spark-network-common_2.11:2.3.2")
//    forcedModules.removeElement("org.apache.spark:spark-streaming_2.11:2.3.2")
    forcedModules = []
  }
}
buildScan {
  termsOfServiceUrl = 'https://gradle.com/terms-of-service'
  termsOfServiceAgree = 'yes'
}
test {
  systemProperty "spark.ui.enabled", "false"
  systemProperty "spark.ui.showConsoleProgress", "false"
  forkEvery 1
  maxParallelForks 4
  useJUnit {
    //TODO add test excludes
  }
}

dependencies {
  shadow "com.fasterxml.jackson.module:jackson-module-scala_2.11:2.9.8"
  shadow project(path: ":beam-model-pipeline", configuration: "shadow")
  shadow project(path: ":beam-sdks-java-core", configuration: "shadow")
  shadow project(path: ":beam-runners-core-construction-java", configuration: "shadow")
  shadow project(path: ":beam-runners-core-java", configuration: "shadow")
  shadow library.java.guava
  shadow library.java.slf4j_api
  shadow library.java.joda_time
  provided library.java.spark_sql
  provided library.java.commons_compress
  provided library.java.commons_lang3
  provided library.java.commons_io_2x
  provided library.java.hamcrest_core
  provided library.java.hamcrest_library
  shadowTest project(path: ":beam-sdks-java-core", configuration: "shadowTest")
  shadowTest project(path: ":beam-runners-core-java", configuration: "shadowTest")
  shadowTest library.java.junit
  shadowTest library.java.mockito_core
  validatesRunner project(path: ":beam-sdks-java-core", configuration: "shadowTest")
  validatesRunner project(path: project.path, configuration: "shadowTest")
  validatesRunner project(path: project.path, configuration: "shadow")
  validatesRunner project(path: project.path, configuration: "provided")
}

configurations.testRuntimeClasspath {
  // Testing the Spark runner causes a StackOverflowError if slf4j-jdk14 is on the classpath
  exclude group: "org.slf4j", module: "slf4j-jdk14"
}

configurations.validatesRunner {
  // Testing the Spark runner causes a StackOverflowError if slf4j-jdk14 is on the classpath
  exclude group: "org.slf4j", module: "slf4j-jdk14"
}

